# -*- coding: utf-8 -*-
"""Osteoporosis_Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17LmptYbpzmR7aaXvOfbQc6abP_raoCUC

# Osteoporosis Project

Model building

# Install PyCaret
"""

#!pip install pycaret

"""# Imports"""

from pycaret.utils import enable_colab
enable_colab()

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""# Load the Data (Add Path to Clean Data File here)

Load the Osteoporosis Data and display the information.


"""

data_path = 'https://raw.githubusercontent.com/canaleal/SystemPortfolio/main/Clean_Data_Main.csv'
dataset  = pd.read_csv(data_path)

"""# Data Analysis

Data analysis is a process of inspecting, cleansing, transforming, and modelling data with the goal of discovering useful information, informing conclusions, and supporting decision-making.
"""

print('Number of rows in the dataset: {}.'.format(dataset.shape[0]))
print('Number of columns in the dataset: {}.'.format(dataset.shape[1]))

dataset.info()

"""# Cleaning Missing Data

Handling the missing values is one of the greatest challenges faced by analysts, because making the right decision on how to handle it generates robust data models. There are a few ways to fix missing values:

**Deleting Rows**
This method is advised only when there are enough samples in the data set. One has to make sure that after we have deleted the data, there is no addition of bias. Removing the data will lead to loss of information which will not give the expected results while predicting the output.

**Replacing With Mean/Median/Mode** 
We can calculate the mean, median or mode of the feature and replace it with the missing values. This is an approximation which can add variance to the data set. But the loss of the data can be negated by this method which yields better results compared to removal of rows and columns. 

**We can also predict the missing values or even assign a unique value.**
"""

missing = pd.DataFrame(dataset.isnull().sum(), columns=['Total'])
missing['%'] = (missing['Total']/dataset.shape[0])*100
missing.sort_values(by='%', ascending=False)

size = dataset.shape[0]
dataset = dataset.dropna()

print('Number of rows in the dataset after the rows with missing values were removed: {}.\n{} rows were removed.'
      .format(dataset.shape[0], size-dataset.shape[0]))

dataset.describe().T

"""# Outliers

When modeling, it is important to clean the data sample to ensure that the observations best represent the problem.

Sometimes a dataset can contain extreme values that are outside the range of what is expected and unlike the other data. These are called outliers and often machine learning modeling and model skill in general can be improved by understanding and even removing these outlier values.
"""

plt.figure(figsize=(20,15))
sns.boxplot(data=dataset)
plt.yticks(range(0,460,30))
plt.xticks(size=15, rotation=55)
plt.title('Data Distribution', size=20, y=1.02)
plt.show()

"""We see that there are some serious outliers in the residual sugar, free sulfur dioxide and total sulfur dioxide columns. Let's remove anything that is above three standard deviations from the mean of the column.

Lets create a function to remove the outliers
"""

cols = ['bmdtest_height', 'bmdtest_weight']

for c in cols:
    upper_level = dataset[c].mean() + 3*dataset[c].std()
    lower_level = dataset[c].mean() - 3*dataset[c].std()
    dataset = dataset[(dataset[c] > lower_level) & (dataset[c] < upper_level)]

print('Number of rows in the dataset after the rows with missing values were removed: {}.\n{} rows were removed.'
      .format(dataset.shape[0], size-dataset.shape[0]))

plt.figure(figsize=(20,15))
sns.boxplot(data=dataset)
plt.yticks(range(0,300,20))
plt.xticks(size=15, rotation=55)
plt.title('Data Distribution after outliers clean up', size=20, y=1.02)
plt.show()

"""# Data Correlation

In statistics, correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data. In the broadest sense correlation is any statistical association, though it actually refers to the degree to which a pair of variables are linearly related.
"""

plt.figure(figsize=(25, 25))
sns.heatmap(dataset.corr(), annot=True, cmap='Reds')
plt.title('Attributes Correlation', y=1.03)
plt.show()

"""# Select Sample

Before we create a model, lets select a sample dataset. Let's split the data 90% for train set and 10% for the test set.
"""

data = dataset.sample(frac=0.9, random_state=786)
data_unseen = dataset.drop(data.index)

data.reset_index(drop=True, inplace=True)
data_unseen.reset_index(drop=True, inplace=True)

print('Data for Modeling: ' + str(data.shape))
print('Unseen Data For Predictions: ' + str(data_unseen.shape))

"""# Creating Model (Setup) Press Enter when the Pycaret says 'Are these columns correct'


Now it's time to create a model which will be trained on the dataset and afterwards will predict the bmd t-score of the data based on its attributes.

The PyCaret library will be used on order to find the best regression model for our dataset.
"""

from pycaret.regression import *
exp_name = setup(data = dataset,  target = 'bmdtest_tscore_fn', silent = True, session_id=123)

"""# Comparing Models

"""

best_model = compare_models(exclude = ['ransac'], n_select = 10)

print(best_model[0])

"""It looks like the Bayesian Ridge model yeilds best results with the dataset, achieveing an RMSE of 0.8892 and very good MAE, MSE, R2 scores. """